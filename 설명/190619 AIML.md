# 190619 AI/ML

## AI/ML

### 딥러닝 기반의 자연어 처리기능을 갖는 챗봇 구현

+ 자연어 처리 기능 구현
  + 텍스트 데이터 전 처리
  + 텍스트 tokenization
  + 텍스트 vectorization
  + 텍스트 embedding 등등
+ DNN(Deep Neural Network) 구조 파악 및 이를 활용한 챗봇 모델
  + 트레이닝 데이터 전처리
  + Language Model



[쉽게 이해하는 딥러닝 영상](https://youtu.be/aF03asAmQbY)

### 머신러닝

머신러닝 

+ 인공지능의 연구분야 중 하나로, 인간의 학습 능력과 같은 기능을 컴퓨터에서 실현하고자 하는 기술 및 기법
+ 순서나 이유를 명확하게 설명하지 못하는 일을 처리하기 위한 방법
+ like 고양이나 강아지 구분 / 기분이 안좋아 보인다 / 컴퓨터에게 아이큐테스트 하는 느낌





### 머신러닝의 분류

1. 지도학습

   + Label이 있는 학습 데이터를 이용해서 학습

   1. 분류 (Classification)

      + KNN
      + SVM
      + Decision Tree
      + Logistic 

      |      | 분류(Classification)                          | 회귀(Regression)                                            |
      | ---- | --------------------------------------------- | ----------------------------------------------------------- |
      | 결과 | 학습 데이터의 레이블 중 하나를 예측(discrete) | 연속된 값을 예측<br>얼마까지 오르게 될까? ex)부동산 집값 등 |

   2. 예측

2. 비지도학습

   + Label이 있는 학습 데이터를 이용해서 학습(?)
   + 비슷한 특성을 갖는 데이터로 묶기
   + 현업에서는 아직 사용하기 어렵다.

   1. 군집
      + 레이블이 없다. 
      + 구매자 유형 분류, 의학 임상 실험 환자군 구별
   2. 이상탐지(Anomaly detection)
      + 기존 데이터 패턴과 다른 이상치 검출
   3. 시각화(visualization)
      + 데이터의 특성을 시각화하여 데이터의 패턴 연구
   4. 차원 축소

3. 강화학습

   + 어떤 주어진 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동 중 보상을 최대화 하는행동 혹은 행동 순서를 선택하는 방법
   + ex) 바둑(알파고)

   1. 과적합(overfitting)
      + 학습 데이터에 너무 지나치게 맞추다보면 일반화 성능이 떨어지는 모델 얻게된다.
      + 학습 데이터의 일부를 검증용(validation)으로 사용하거나 교차검증, 데이터셋 늘리기 사용

   





## 자연어처리(NLP)(Natural Language Process)

### 자연어 처리

+ 자연어 처리
  + 컴퓨터 과학, 인공지능과 언어학이 합쳐진 분야로 자연어를 컴퓨터에서 분석하고 처리
+ 처리 목적
  + 대량의 자연어 처리(문자 종류 빈포, 언어 종류 수, 문서 분류)
  + 컴퓨터에 자연어를 이해시키는 자연어 이해 시스템



#### 자연어 처리 모델(classical NLP & Deep Learning)

+ 앞단의 전처리 과정 NLP
+ 학습은 딥러닝이



### 전처리 기술

1. Noise canceling : 스펠링 체크 및 띄어씍 오류 교정 => 결국 이부분은 사람이 해야한다.
2. Tokenizing : 문장으로 토큰으로 나눔, 토큰은 n-gram, 어절, 단어 등으로 목적에 따라 다르게 정의
3. part-of-Speech tagging : 주어진 토큰의 품사 판별
4. filtering : 불필요한 단어 제거
5. Term vector representation : (문서, 단어) 행렬에서 각 단어의 중요도를 조절 => 영어같은 경우는 앞부분이 중요. 한글은 뒷부분이 중요(뒷부분에 중요 내용이 나온다.)
6. Transformation : TF-IDF 등의 방식으로 term vector 변환







## 딥러닝(Deep Learning)

### 딥러닝

+ 딥러닝?

  + 컴퓨터가 스스로 학습할 수 있게 하기 위해 인공 신경망을 기반으로 하는 기계 학습
  + 인간의 신경망(Neural Network) 이론을 이용한 인공 신경망(ANN, Artificial Neural Network)의 일종으로 계층 구조로 구성되면서 입력층과 출력층 사이에 하나 이상의 은닉층을 가지고 있는 심층 심경망(DNN, Deep Nueral Network)이다.

+ 왜 필요한가?

  + 가능한 모든 경우를 찾아도 예외가 발생하게된다.
  + 그러나 이것이 발생하지 않는다.

+ 퍼셉트론?

  + 사람의 뇌. 인간의 뉴런과 유사한 동작을 할 수 있게 하기 위해서 들어오는 입력에 따른 가중치를 곱한 값에 어떠한 함수를 계산. 이후 바이어스를 더한 값이 출력값이 된다.
  + y = w1x1 + w2x2 + b

+ 작동 원리 예시

  + input 차원에서 직선 혹은 평면을 의미하는 수식

+ 다중 퍼셉트론(Multi Layer Perceptron, MLP)

  + 다층 퍼셉트론으로 XOR 연산이 가능

  + 여전히 남은 문제점

    1. 비선형 분류의 어려움

    2. 다층으로 쌓아올린 퍼셉트론의 학습 방법 부재

       => 빙하기 돌입

+ Back Propagation

  + 기존의 계산 값을 앞으로 나가는 output 쪽으로만의 계산이 아닌, 다시 뒷단으로 넘어가서 계산했다가 앞으로 나가게 계산하게 하는것을 의미
  + 앞으로 갔다가 뒤로 못오는 경우가 발생할 수 있기 때문에 잘 체크하기 => vanishing gradient solution



[딥러닝 영상](https://youtu.be/kMGEpIYPCiM)







